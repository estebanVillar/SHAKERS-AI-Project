# Frontend and Evaluation Part 2: Metrics and Automated Evaluation

## 1. Performance Monitoring

A production-grade system requires visibility into its performance. This project addresses this need through two key components: comprehensive logging and a user-facing dashboard.

### 1.1. Query Logging: The Source of Truth

The foundation of all monitoring is the **query log**. For every call made to the primary `/api/query` endpoint, a structured JSON line is appended to the `query_logs.jsonl` file.

*   **Why `.jsonl`?** The JSON Lines format is used because it is an append-only format, which is perfect for logging. Each line is a self-contained, valid JSON object, which is easy to read line-by-line without loading the entire file into memory. It is also more robust against corruption than a single large JSON array.

*   **Logged Fields:** Each log entry captures a rich set of data for a single transaction:
    *   `timestamp`: The UTC timestamp of the query, for time-series analysis.
    *   `user_id`: Links the query to a specific user.
    *   `query`: The raw text of the user's question.
    *   `answer`: The final answer generated by the assistant.
    *   `sources`: The list of source document topics used to generate the answer. This is vital for analyzing retrieval accuracy.
    *   `latency_ms`: The total time taken, in milliseconds, for the endpoint to process the request. This is the key performance metric.

### 1.2. The Metrics Dashboard

The data from the query log is made accessible and understandable through the "Metrics" page in the Streamlit UI, powered by the `app/metrics_page.py` script.

*   **Functionality:** This page reads the `query_logs.jsonl` file into a `pandas` DataFrame and computes several key performance indicators (KPIs) in real-time.
*   **Displayed Metrics:**
    1.  **Core Metrics (`st.metric`):**
        *   `Total Queries`: A simple count of all queries processed.
        *   `Avg. Latency (ms)`: The average of the `latency_ms` column, providing an at-a-glance view of system speed.
        *   `Unique Sources`: Counts the number of unique documents ever cited, indicating the breadth of knowledge base usage.
    2.  **Performance Over Time (`st.line_chart`):** A line chart that plots latency on the y-axis against the timestamp on the x-axis, making it easy to spot performance degradations or improvements over time.
    3.  **Recent Queries Log (`st.dataframe`):** A table showing the last 10 queries, allowing for a quick inspection of recent activity.

---

## 2. Automated Evaluation (`evaluation.py`)

While the dashboard provides real-time monitoring, the `evaluation.py` script (as planned) is designed to provide a deep, objective, and repeatable assessment of the system's *quality*. It functions as an automated integration testing suite.

### 2.1. RAG System Evaluation: Measuring Accuracy

This part of the script will assess the core question-answering capabilities.

*   **Input:** It uses a ground-truth dataset, `data/evaluation/qa_dataset.json`, which contains question-answer pairs. Each entry includes the question, keywords expected in the answer, and the exact source topics that should be retrieved.
*   **Automated Metrics:**
    *   **Answer Score (Keyword Match):** This will measure content accuracy by checking for the presence of the `ideal_answer_keywords` in the generated response. It's a proxy for correctness.
    *   **Retrieval Score (Source Recall):** This measures retrieval precision by comparing the `sources` returned by the API against the `expected_sources` in the dataset.

### 2.2. Recommendation System Evaluation: Measuring Relevance

This part will test the personalization engine's ability to predict user interests.

*   **Input:** It uses the `data/evaluation/user_profiles.json`, which contains simulated user query histories.
*   **Methodology: "Hold-One-Out" Cross-Validation**
    This sophisticated approach simulates a user's journey. For a user with queries [Q1, Q2, Q3], the script will:
    1.  Prime the system with Q1.
    2.  Get recommendations.
    3.  Check if the recommendations include the topic of the *next* query, Q2.
    4.  Then, prime the system with Q2, get new recommendations, and check if they predict Q3.
*   **Automated Metric:**
    *   **Recommendation Hit Rate:** This will be calculated as the overall percentage of times the system successfully recommended the topic of the user's next logical query. This is a powerful, objective measure of relevance.