# Implementation: Metrics and Evaluation

## 1. Overview: Measuring Success
A production-grade AI system requires robust methods for monitoring performance and evaluating quality. This project addresses this need through two key mechanisms:
1.  **Real-Time Performance Monitoring:** A comprehensive logging system and a user-facing dashboard to track key performance indicators (KPIs) like latency and usage in real-time.
2.  **Automated Quality Evaluation:** A dedicated script (`evaluation.py`) designed to provide a deep, objective, and repeatable assessment of the system's *quality* and *accuracy*.

---

## 2. Real-Time Performance Monitoring

### 2.1. The Foundation: Structured Logging
The foundation of all monitoring is the **query log**. For every call made to the primary `/api/query` and `/api/get_document` endpoints, a structured JSON line is appended to the `query_logs.jsonl` file.

*   **Why `.jsonl`?** The JSON Lines format is used because it is an append-only format, perfect for logging. Each line is a self-contained, valid JSON object, which is easy to process without loading the entire file into memory and is robust against corruption.

*   **Logged Fields:** Each log entry captures a rich set of data for a single transaction:
    *   `timestamp`: The UTC timestamp of the query.
    *   `user_id`: Links the query to a specific user session.
    *   `query`: The raw text of the user's question.
    *   `answer`: The final answer generated by the assistant.
    *   `sources`: The list of source document topics used to generate the answer, vital for analyzing retrieval accuracy.
    *   `latency_ms`: The total server-side time taken, in milliseconds, to process the request. This is the key performance metric.

### 2.2. The Metrics Dashboard (`metrics_page.py`)
The data from the query log is made accessible and understandable through the "Metrics" page in the Streamlit UI, which is rendered by the `app/metrics_page.py` script.

*   **Functionality:** This page reads the `query_logs.jsonl` file into a `pandas` DataFrame and computes several KPIs in real-time.
*   **Displayed Metrics:**
    *   **Core Metrics (`st.metric`):** `Total Queries`, `Avg. Latency (ms)`, and `Unique Sources` cited.
    *   **Performance Over Time (`st.line_chart`):** A line chart plotting `latency_ms` against `timestamp` to spot performance trends.
    *   **Recent Queries Log (`st.dataframe`):** A table showing the last 10 queries for quick inspection.
*   **Feedback Metrics:** The dashboard also reads `feedback_logs.jsonl` to display statistics on user feedback, including counts of "helpful" vs. "unhelpful" ratings and the overall satisfaction score.

---

## 3. Automated Quality Evaluation (`evaluation.py`)
While the dashboard provides real-time monitoring, the `evaluation.py` script provides an objective, offline assessment of the system's quality against a ground-truth dataset. It functions as an automated integration testing suite for the AI.

### 3.1. RAG System Evaluation
This part of the script assesses the core question-answering capabilities.
*   **Input:** It uses a ground-truth dataset, `data/evaluation/qa_dataset.json`, which contains question-answer pairs. Each entry includes the question, keywords expected in the answer, and the exact source topics that *should* be retrieved.
*   **Metrics:**
    *   **Answer Score (Keyword Match):** Measures content accuracy by checking for the presence of the `ideal_answer_keywords` in the generated response.
    *   **Retrieval Score (Source Recall):** Measures retrieval precision by comparing the `sources` returned by the API against the `expected_sources` in the dataset.

### 3.2. Recommendation System Evaluation
This part tests the personalization engine's ability to predict user interests.
*   **Methodology: "Hold-One-Out" Cross-Validation:** This sophisticated approach simulates a user's journey. For a user with a query history of [Q1, Q2, Q3], the script will:
    1.  Prime the system by making an API call for Q1 to build a user profile.
    2.  Get recommendations based on that profile.
    3.  Check if the recommendations include the topic of the *next actual query*, Q2.
    4.  It repeats this process, using Q2 to predict Q3, and so on for all users.
*   **Metric:**
    *   **Recommendation Hit Rate:** The overall percentage of times the system successfully recommended the topic of the user's next logical query. This is a powerful, objective measure of relevance.

---
## 4. Design Rationale
This dual approach to evaluation provides a complete picture of the system's health. The real-time dashboard is essential for operational monitoring and identifying immediate issues like high latency. The offline evaluation script is crucial for regression testing and ensuring that changes to the system (like new prompt strategies or updated models) lead to quantifiable improvements in the quality and accuracy of the AI's responses.