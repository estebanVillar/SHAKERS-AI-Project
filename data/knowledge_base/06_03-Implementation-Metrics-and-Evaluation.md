# Implementation: Metrics and Evaluation

## 1. Overview: A Two-Pronged Approach to Measuring Success
A production-grade AI system requires more than just a functional output; it needs robust methods for monitoring its performance and evaluating its quality. This project addresses this critical requirement through two distinct but complementary mechanisms, directly fulfilling the case study's stipulations for a metrics dashboard and evaluation components.

1.  **Real-Time Performance and Cost Monitoring:** A live, user-facing dashboard in the Streamlit application that provides real-time insights into key performance indicators (KPIs) like latency, cost, and usage. This is for *operational awareness*.
2.  **Automated Quality Evaluation:** A dedicated, offline Python script (`evaluation.py`) designed to provide a deep, objective, and repeatable assessment of the system's *quality* and *accuracy* against a ground-truth dataset. This is for *AI quality assurance*.

---

## 2. Real-Time Performance Monitoring (`metrics_page.py`)

### 2.1. The Foundation: Structured Logging
The foundation of all monitoring is the **query log**. For every single call made to the `/api/query` and `/api/get_document` endpoints, a structured JSON line is appended to the `query_logs.jsonl` file.

*   **Why `.jsonl`?** The JSON Lines format is used because it is an append-only format, making it perfect for concurrent logging without the risk of file corruption that can occur with a single large JSON array. Each line is a self-contained, valid JSON object, which is easy and memory-efficient to process line-by-line.

*   **Logged Fields:** Each log entry is a rich snapshot of a single transaction:
    *   `timestamp`: The precise UTC timestamp of the query.
    *   `user_id`: Links the query to a specific user session.
    *   `query`: The raw text of the user's question.
    *   `answer`: The final answer generated by the assistant.
    *   `sources`: The list of source document topics used, vital for analyzing retrieval accuracy.
    *   `latency_ms`: The total server-side time taken to process the request, a key performance metric.
    *   `input_tokens`, `output_tokens`, `total_tokens`: Token counts from the LLM call.
    *   `cost`: The estimated monetary cost of the LLM call, calculated on the backend.

### 2.2. The Metrics Dashboard
The data from the log files is made accessible and understandable through the "Metrics" page in the Streamlit UI, rendered by the `app/metrics_page.py` script.

*   **Functionality:** This page reads `query_logs.jsonl` and `feedback_logs.jsonl` into `pandas` DataFrames and computes several KPIs in real-time.
*   **Displayed Metrics:**
    *   **Core KPIs (`st.metric`):** `Total Queries`, `Avg. Latency (ms)`, `Total Tokens`, and `Estimated Cost (USD)`.
    *   **Performance Over Time (`st.line_chart`):** A chart plotting latency, tokens, and cost against time to spot performance trends or costly queries.
    *   **Feedback Metrics:** A section showing `Total Feedback`, `Helpful` counts, and a `Satisfaction Score` calculated from user feedback.
    *   **Live Log (`st.dataframe`):** A table showing the last 10 queries for quick inspection and debugging.

---

## 3. Automated Quality Evaluation (`evaluation.py`)
While the dashboard monitors live performance, the `evaluation.py` script provides an objective, offline assessment of the AI's core quality. It functions as an automated integration testing suite for the AI components.

### 3.1. RAG System Evaluation
This part assesses the core question-answering capabilities against a predefined "gold standard."
*   **Input:** It uses a ground-truth dataset, `data/evaluation/qa_dataset.json`, which contains question-answer pairs. Each entry includes a question, a list of keywords expected in a correct answer, and the exact source topics that *should* be retrieved.
*   **Process:** The script iterates through each question, calls the `/api/query` endpoint, and compares the generated response against the ground truth.
*   **Metrics:**
    *   **Answer Score (Keyword Match):** Measures content accuracy by checking for the presence of the `ideal_answer_keywords` in the generated response.
    *   **Retrieval Score (Source Recall):** Measures retrieval precision by comparing the `sources` returned by the API against the `expected_sources` in the dataset. This is a direct measure of the retriever's effectiveness.

### 3.2. Recommendation System Evaluation
This part tests the personalization engine's ability to predict a user's next interest.
*   **Methodology: "Hold-One-Out" Simulation:** This sophisticated approach simulates a user's conversational journey. It uses predefined user histories from `evaluation_user_profiles.json`. For a user with a query history of [Q1, Q2, Q3], the script will:
    1.  Prime the system by making an API call for Q1 to build an initial user profile.
    2.  Get recommendations based on that profile.
    3.  Check if the recommended topics include the topic related to the *next actual query*, Q2.
    4.  It then repeats this process, using the history up to Q2 to predict Q3, and so on.
*   **Metric:**
    *   **Recommendation Hit Rate:** The overall percentage of times the system successfully recommended the topic of the user's next logical query. This is a powerful and objective measure of the recommendation engine's relevance.

---

## 4. Design Rationale
This dual approach to evaluation provides a complete, 360-degree view of the system's health and quality. The real-time dashboard is essential for operational monitoring, debugging, and identifying immediate issues like high latency or cost overruns. The offline evaluation script is crucial for regression testing, ensuring that changes to the system (like new prompt strategies, updated models, or different retriever settings) lead to quantifiable improvements in the quality and accuracy of the AI's core functions.
