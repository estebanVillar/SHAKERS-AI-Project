# Welcome to the Shakers Intelligent Support System! ðŸ‘‹

This documentation provides a deep dive into the architecture and implementation of the Shakers Intelligent Support System, a robust backend solution designed to revolutionize user interaction within the Shakers ecosystem. Built with a focus on clarity, accuracy, and personalization, this system leverages advanced AI models and a curated knowledge base to deliver instant, accurate answers and proactive, personalized recommendations.

## Architectural Foundation: A Python Flask Backend

The core of the Shakers Intelligent Support System is a **Python Flask application**. Flask serves as the lightweight web server framework, responsible for handling incoming API requests, orchestrating the underlying AI pipelines, and managing data persistence. This choice provides flexibility and a clear separation of concerns, allowing for efficient development and deployment of the intelligent services.

## Core Components: A Deep Dive into Implementation

The system is composed of two synergistic services, meticulously engineered using a combination of open-source libraries and Google's Generative AI capabilities.

### 1. Retrieval-Augmented Generation (RAG) Query Service

This service is the brain of our question-answering capabilities, built upon the powerful **Retrieval-Augmented Generation (RAG)** paradigm. Its primary function is to provide highly contextual and factual answers by grounding the Large Language Model (LLM) in our proprietary knowledge base.

*   **Orchestration with LangChain:** The entire RAG pipeline is orchestrated using **LangChain**, a framework designed for developing applications powered by language models. LangChain manages the flow from user query to final answer, integrating various components seamlessly.
*   **Embedding Model:** For transforming textual data into numerical vector representations (embeddings), we utilize **GoogleGenerativeAIEmbeddings** with the `models/embedding-001` model. These embeddings capture the semantic meaning of text, enabling efficient similarity searches.
*   **Vector Store (FAISS):** The vectorized document chunks from our knowledge base are stored and retrieved using **FAISS (Facebook AI Similarity Search)**. FAISS is an open-source library that provides highly optimized algorithms for similarity search in large datasets of vectors. This allows the system to quickly find the most relevant information when a user poses a question. The FAISS index is persisted to disk (`faiss_index`) and loaded on startup for performance.
*   **Large Language Model (LLM):** The generative component of our RAG system is powered by **GoogleGenerativeAI**, specifically the `gemini-1.5-flash-latest` model. This LLM is responsible for synthesizing the retrieved information into a coherent, human-like answer.
*   **RAG Process Flow:**
    1.  **Document Loading:** Markdown files from the `data/knowledge_base` directory are loaded using LangChain's `DirectoryLoader` and `TextLoader`.
    2.  **Text Splitting:** Loaded documents are broken down into smaller, manageable `chunks` using `RecursiveCharacterTextSplitter`. This ensures that each chunk is small enough for efficient embedding and LLM processing, while maintaining contextual integrity.
    3.  **Embedding & Indexing:** These chunks are then embedded using `GoogleGenerativeAIEmbeddings` and indexed into the FAISS vector store.
    4.  **Retrieval:** When a user submits a query to the Flask application's `/api/query` endpoint, the query is also embedded. A similarity search is performed against the FAISS index to retrieve the top `k` (currently 5) most relevant document chunks.
    5.  **Prompt Augmentation & Generation:** The retrieved chunks, along with the user's original query and any chat history, are then combined into a carefully constructed prompt (`ChatPromptTemplate`). This augmented prompt is fed to the `gemini-1.5-flash-latest` LLM, which is explicitly instructed to generate an answer *only* based on the provided context. This strict adherence to the retrieved information is crucial for minimizing "hallucinations" and ensuring factual accuracy.
    6.  **Source Citation:** The system is designed to cite the `Source Topic` (derived from the filename of the knowledge base document) for each piece of information used in the generated answer, enhancing transparency and verifiability.

### 2. Personalized Recommendation Service

This service transforms the user experience from reactive to proactive, acting as an intelligent guide that anticipates user needs.

*   **Dynamic User Profiles:** The system maintains dynamic user profiles, stored persistently in `data/evaluation/user_profiles.json`. Each profile tracks a user's `query_history` and, crucially, their `inferred_interests`.
*   **Interest Inference:** The `inferred_interests` are automatically populated based on the `Source Topic` of the documents retrieved by the RAG service for a user's queries. This means that as a user asks questions, the system learns about their areas of interest.
*   **Recommendation Logic:** When a request is made to the Flask application's `/api/recommendations` endpoint, the system analyzes the user's `inferred_interests`. It then identifies topics within the knowledge base that the user has *not yet explored* (i.e., topics not present in their `inferred_interests`). These "novel" topics are prioritized for recommendation. If all inferred interests have been covered, the system suggests general, high-value topics from the knowledge base to broaden the user's understanding.
*   **Explainable Recommendations:** Each recommendation is accompanied by a brief explanation, making the suggestion transparent and helpful for the user.

## Data Management and Persistence

The system relies on file-based persistence for key data:

*   **`data/knowledge_base/`:** Contains all the Markdown documents that form the system's knowledge base.
*   **`data/evaluation/user_profiles.json`:** Stores the dynamic user profiles, including query history and inferred interests.
*   **`query_logs.jsonl`:** A line-delimited JSON file that logs every RAG query, its answer, sources, and latency for performance monitoring and debugging.

All file operations are safeguarded using a `threading.Lock` to ensure thread safety in the multi-threaded Flask environment.

## Key Technologies Stack

*   **Python:** The primary programming language.
*   **Flask:** Web server framework for the backend API.
*   **LangChain:** Framework for building and orchestrating LLM applications.
*   **Google Gemini API:** Provides the `GoogleGenerativeAIEmbeddings` and `GoogleGenerativeAI` (Gemini 1.5 Flash) models.
*   **FAISS:** Efficient library for similarity search and vector storage.
*   **`python-dotenv`:** For managing environment variables (e.g., `GOOGLE_API_KEY`).

This comprehensive overview sets the stage for understanding the detailed guides and API references that follow, providing both a high-level understanding and insights into the underlying technical implementation.