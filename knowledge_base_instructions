
To the Gemini CLI: Please use the following instructions to generate a series of markdown files. Each section defines a file to be created, its path, and the content it should contain. Analyze the provided project source code (C:\Users\Esteban Fernandez\Desktop\WORK\SHAKERS AI Project\app\main.py, app\Chat_app.py, app\metrics_page.py) to generate detailed explanations based on the skeletons.

Please be as descriptive as possible, with a lot of detail.

FILE: knowledge_base/00-Project-Overview.md

DESCRIPTION:
This is the top-level introduction to the project. It should provide a high-level summary, outlining the project's purpose, core features, and the technologies used. It's the starting point for anyone trying to understand the project.

CONTENT_SKELETON:

Generated markdown
# 1. Project Overview & Core Mission
   - Explain the main goal: to create an intelligent technical support system with personalized recommendations about its own architecture.
   - Describe the dual objective: answering technical questions based on a knowledge base and proactively recommending relevant documentation.

# 2. Key Features
   - **Conversational RAG System:** Briefly describe the chatbot that answers questions.
   - **Personalized Recommendations:** Explain the system that suggests new topics based on user interaction.
   - **Intelligent Routing:** Mention the optimization for direct questions versus conversational ones.
   - **Metrics Dashboard:** Describe the page for monitoring performance.

# 3. Technology Stack
   - **Backend:** Flask, LangChain, Google Gemini (LLM & Embeddings), FAISS (Vectorstore).
   - **Frontend:** Streamlit.
   - **Core Language:** Python.

# 4. Project File Structure
   - Provide a brief explanation of the key directories and files:
     - `app/`: The main application code.
     - `app/main.py`: The Flask backend server.
     - `app/Chat_app.py`: The Streamlit frontend UI.
     - `knowledge_base/`: The source documents for the RAG system (this directory).
     - `data/`: For logs and user profiles.
     - `evaluation.py`: The script for automated testing.


FILE: knowledge_base/rag-pipeline/01-Ingestion-and-Indexing.md

DESCRIPTION:
This file details the first critical step of the RAG pipeline: how the raw markdown files from the knowledge_base are loaded, processed, split, and stored in a searchable index.

CONTENT_SKELETON:

Generated markdown
# 1. Overview of the Ingestion and Indexing Pipeline
   - Explain that this is a one-time process (with caching) that prepares the documents for retrieval.

# 2. Document Loading
   - Describe the use of `langchain_community.document_loaders.DirectoryLoader` to load all `.md` files from the specified knowledge base path.

# 3. The Parent Document Retriever Splitting Strategy
   - This is a key feature. Explain the concept clearly.
   - **Parent Splitter (`RecursiveCharacterTextSplitter`):** Describe how documents are first split into larger, logically coherent parent chunks (chunk_size=2000).
   - **Child Splitter (`RecursiveCharacterTextSplitter`):** Describe how these parent chunks are then split into smaller child chunks (chunk_size=400). Explain that **only these small child chunks are vectorized**.
   - **Why this strategy is used:** Explain that it allows for highly specific vector searches on small chunks while retrieving the larger, more contextually complete parent chunk for the LLM to read.

# 4. Vectorization and Storage
   - **Embeddings Model:** Specify the use of `GoogleGenerativeAIEmbeddings` with the `models/embedding-001` model.
   - **Vectorstore:** Detail the use of `langchain_community.vectorstores.FAISS` to create the index from the *child document* embeddings.
   - **Document Store:** Explain the role of `langchain.storage.InMemoryStore`, which holds the actual content of the *parent documents*, keyed by their IDs.

# 5. Caching for Efficiency
   - Explain how the script checks for the existence of the `faiss_pdr_index` directory and `pdr_docstore.pkl` file in the `app/cache/` directory to avoid re-indexing on every server start.
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Markdown
IGNORE_WHEN_COPYING_END

FILE: knowledge_base/rag-pipeline/02-Conversational-Retrieval-Chain.md

DESCRIPTION:
This file explains how an incoming user query is processed to retrieve relevant documents and generate a final, context-aware answer. It describes the main LangChain "chain" of operations.

CONTENT_SKELETON:

Generated markdown
# 1. The End-to-End Retrieval Chain
   - Provide a high-level overview of the `rag_chain` object and its goal: to take a user query and chat history, find relevant documents, and generate a cited answer.

# 2. Step 1: Contextualizing the Question
   - **`create_history_aware_retriever`:** Explain the purpose of this component.
   - **Recontextualization Prompt:** Describe how the prompt works. It takes the chat history and the new "follow-up" question and instructs the LLM to rewrite it as a standalone question. Provide an example (e.g., "how does it work?" -> "how does the parent document retriever work?").

# 3. Step 2: Retrieving Documents
   - Explain that the newly rephrased, standalone question is then used to query the `ParentDocumentRetriever` (as described in the Ingestion file).

# 4. Step 3: Generating the Final Answer
   - **`create_stuff_documents_chain`:** Explain that this component takes the retrieved documents and the original user query.
   - **The System QA Prompt (`qa_prompt`):** This is critical. Detail the rules embedded in the prompt:
     - The assistant must answer **only** based on the provided context.
     - It **must** cite the sources using the `[topic/name]` format at the end of sentences. The `topic` metadata is derived from the document's file path.
     - If the answer isn't in the context, it **must** respond with the specific "I'm sorry..." message.

# 5. Putting It All Together
   - Briefly describe how `create_retrieval_chain` links the history-aware retriever (Steps 1 & 2) with the question-answer chain (Step 3) to form the complete `rag_chain`.
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Markdown
IGNORE_WHEN_COPYING_END

FILE: knowledge_base/recommendation-system/01-User-Profile-and-Interest-Vector.md

DESCRIPTION:
This file details how the system builds a profile for each user to understand their interests, which is the foundation for providing personalized recommendations.

CONTENT_SKELETON:

Generated markdown
# 1. Purpose of User Profiles
   - Explain that user profiles are used to track user interests over time to provide relevant document recommendations.

# 2. Data Storage
   - Describe the `user_profiles.json` file located in `app/data/evaluation/`.
   - Detail the structure of a user's profile within the JSON file:
     - **`user_id`**: The unique identifier for the user.
     - **`query_history`**: A list of all queries made by the user with timestamps.
     - **`inferred_interests`**: A list of topics (source document paths) the user has already seen based on the documents retrieved for their queries.
     - **`profile_vector`**: The key componentâ€”an evolving numerical representation of the user's interests.

# 3. The Profile Vector
   - **What it is:** Explain that the profile vector is a single embedding vector that represents the average of all the user's past query embeddings.
   - **How it's updated:** Describe the process in the `/api/query` endpoint.
     - 1. A new user query is received and embedded using the same Google Gemini model.
     - 2. If the user has an existing `profile_vector`, the new query vector is averaged with the old vector (`np.mean`).
     - 3. If it's a new user, the first query's vector becomes their initial `profile_vector`.
     - 4. This updated vector is then saved back to the `user_profiles.json` file.
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Markdown
IGNORE_WHEN_COPYING_END

FILE: knowledge_base/recommendation-system/02-Recommendation-Generation-Logic.md

DESCRIPTION:
This document explains the logic within the /api/recommendations endpoint, detailing how the user's profile vector is used to find and rank relevant new documents to suggest.

CONTENT_SKELETON:

Generated markdown
# 1. The Recommendation Goal
   - To suggest 2-3 relevant but previously unconsulted articles to broaden the user's knowledge.

# 2. Pre-computing Document Embeddings
   - Explain the `doc_embeddings_cache` dictionary, which is created on server startup.
   - Describe how the system iterates through every document in the knowledge base, embeds its full content, and stores the embedding in this cache, keyed by the document's topic. Explain that this is a critical optimization to make recommendations fast.

# 3. The Core Recommendation Algorithm
   - **Step 1: Get User Profile:** Retrieve the `profile_vector` and the list of `inferred_interests` for the user.
   - **Step 2: Calculate Similarity:**
     - Loop through every document in the `doc_embeddings_cache`.
     - For each document, calculate the cosine similarity between the user's `profile_vector` and the document's embedding. This score represents how relevant the document is to the user's overall interests.
   - **Step 3: Rank and Filter:**
     - Sort all documents in descending order based on their similarity score.
   - **Step 4: Generate Explained Recommendations:**
     - Iterate through the sorted list to build a list of 3 recommendations.
     - Detail the logic for creating the explanation text, based on the similarity score and whether the topic has been consulted before (is in `inferred_interests`).
     - Example explanations: "Highly relevant to your interests and you haven't seen it yet." or "You've touched on this; a review might offer deeper insights."

# 4. Ensuring Diversity
   - Explain how the logic avoids recommending documents the user has already seen and ensures the recommendations in a single batch are unique.
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Markdown
IGNORE_WHEN_COPYING_END

FILE: knowledge_base/api-reference/01-API-Endpoints-Overview.md

DESCRIPTION:
This file describes the Flask backend API, detailing each available endpoint, its purpose, request/response formats, and how it fits into the overall system.

CONTENT_SKELETON:

Generated markdown
# 1. API Overview
   - Explain that the Flask application in `main.py` serves as the backend, handling all AI logic.
   - All endpoints expect and return JSON.

# 2. Endpoint: `POST /api/query`
   - **Purpose:** The main endpoint for handling conversational queries.
   - **Request Body:**
     - `query` (string, required)
     - `user_id` (string, required)
     - `chat_history` (list, optional)
   - **Workflow:**
     1. Invokes the main `rag_chain`.
     2. Updates the user's profile vector.
     3. Logs the query, response, and latency to `query_logs.jsonl`.
   - **Success Response:**
     - `answer` (string)
     - `sources` (list of strings)

# 3. Endpoint: `POST /api/recommendations`
   - **Purpose:** To get a list of recommended documents for a user.
   - **Request Body:**
     - `user_id` (string, required)
   - **Workflow:** Implements the recommendation generation logic.
   - **Success Response:**
     - `recommendations` (list of objects, each with `title` and `explanation`).

# 4. Endpoint: `POST /api/get_document`
   - **Purpose:** An **optimized route** for directly summarizing a single, known document topic.
   - **Request Body:**
     - `topic` (string, required)
   - **Workflow:**
     1. Bypasses the complex RAG chain.
     2. Directly retrieves the document's content from the `doc_embeddings_cache`.
     3. Uses a simple summarization prompt with the LLM.
     4. This is called by the frontend when a user clicks a recommendation button to improve speed and reduce cost.
   - **Success Response:**
     - `answer` (string, the summary)
     - `sources` (list containing just the requested topic)
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Markdown
IGNORE_WHEN_COPYING_END

FILE: knowledge_base/frontend-and-evaluation/01-Streamlit-Frontend.md

DESCRIPTION:
This file details the user-facing part of the application, Chat_app.py, explaining how it manages state, builds the UI, and interacts with the Flask backend.

CONTENT_SKELETON:

Generated markdown
# 1. Role of the Streamlit App
   - Explain that `Chat_app.py` provides the web-based chat interface for the user.

# 2. Session State Management
   - Describe the use of `st.session_state` to manage application state across user interactions.
   - Detail key session variables:
     - `current_page`: To switch between "chat" and "metrics".
     - `chat_sessions`: A dictionary holding all chat histories.
     - `current_chat_id`: To identify the active chat.
     - `user_id`: A persistent ID for the user across sessions.

# 3. UI Components
   - **Sidebar:** Describe the navigation for "New Chat", "Chat", "Metrics", and the list of past chat histories.
   - **Main Chat Interface:** Explain how it displays the messages, the list of recommendations as clickable buttons, and the `st.chat_input` field.

# 4. Intelligent Frontend Routing
   - This is a key optimization feature.
   - Explain the `re.search(r"Tell me about\s+(.+)", prompt)` logic.
   - Describe how, if a user's prompt matches this pattern (often triggered by a recommendation button), the frontend calls the dedicated, faster `/api/get_document` endpoint instead of the standard `/api/query` endpoint.

# 5. Connecting to the Backend
   - Show how the app uses the `requests` library to make POST calls to the Flask API endpoints (`/api/query`, `/api/recommendations`).
   - Explain the error handling for `requests.exceptions.ConnectionError` to gracefully handle cases where the backend server is not running.
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Markdown
IGNORE_WHEN_COPYING_END

FILE: knowledge_base/frontend-and-evaluation/02-Metrics-and-Evaluation.md

DESCRIPTION:
This document covers how the project's performance is monitored and evaluated, explaining both the live metrics dashboard and the offline automated evaluation script.

CONTENT_SKELETON:

Generated markdown
# 1. Performance Monitoring

## 1.1. Query Logging
   - Describe the `query_logs.jsonl` file.
   - Explain its purpose: to create a structured, append-only log of every query processed by the system.
   - Detail the fields logged for each entry: `timestamp`, `user_id`, `query`, `answer`, `sources`, and `latency_ms`.

## 1.2. The Metrics Dashboard
   - Explain that `metrics_page.py` provides the code for the dashboard.
   - Describe how it reads the `query_logs.jsonl` file using pandas.
   - List the key metrics displayed on the dashboard:
     - Total Queries
     - Average Latency (ms)
     - Unique Sources Cited
     - A line chart of latency over time.
     - A table of the most recent queries.

# 2. Automated Evaluation (`evaluation.py`)
   - Explain the purpose of this script: to provide an objective, repeatable way to measure the quality of the system against a ground-truth dataset.

## 2.1. RAG System Evaluation
   - **Input:** `qa_dataset.json` (a list of questions with ideal keywords and expected sources).
   - **Metrics:**
     - **Answer Score:** Calculated by matching keywords from the ideal answer in the generated answer.
     - **Retrieval Score:** Measures how accurately the retrieved sources match the expected sources.

## 2.2. Recommendation System Evaluation
   - **Input:** `user_profiles.json` (simulated user histories).
   - **Method:** "Hold-One-Out" cross-validation. For each query in a user's history, it checks if the recommendations generated can predict the topic of the *next* query.
   - **Metric:**
     - **Recommendation Hit Rate:** The percentage of times a correct future topic was successfully recommended.
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Markdown
IGNORE_WHEN_COPYING_END